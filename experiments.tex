\section{Experiments: compositionality}

We begin with a set of motivating experiments on synthetic data.
Compositionality, and the corresponding ability to answer questions with
arbitrarily complex structure, is an essential part of the kind of deep image
understanding visual QA datasets are intended to test. At the same time,
questions in most existing natural image datasets are quite simple, for the most
part requiring that only one or two pieces of information be extracted from an
image in order to answer it successfully, and with little evaluation of
robustness in the presence of distractors (e.g. asking \emph{is there a blue
house} in an image containing a red house and a blue car).

As one of the primary goals of this work is to learn models for deep semantic
compositionality, we have created \shapes, a synthetic dataset that places such
compositional phenomena at the forefront. This dataset consists of complex
questions about simple arrangements of colored shapes (\autoref{fig:examples}).  Questions contain
between two and four attributes, object types, or relationships.  There are 244
questions and 15616 images in total, divided into train and test sets (with no
overlap between the train and test sets). To eliminate mode-guessing as a
viable strategy, all questions have a yes-or-no answer, but good performance
requires that the system learn to recognize shapes and colors, and understand
both spatial and logical relations among sets of objects.

While success on this dataset is by no means a sufficient condition for robust
visual QA, we believe it is a necessary one. In this respect it is similar in
spirit to the bAbI \cite{Weston15BABI} dataset, and we hope that \shapes will
continue to be used in conjunction with natural image datasets.

%Past work [Ren paper] has achieved state-of-the-art results on the \cocoqa
%dataset using a bag-of-words model. This is consistent with [VQA paper]'s
%observation that image features are most important for simple object- and
%activity-recognition questions, and that they make little difference for other
%categories (like yes/no questions). Taken together, these results suggest that
%existing natural image datasets primarily require simple object- and attribute
%recognition, with limited importance attached to spatial relations and other
%highly compositional phenomena.


%\begin{figure}
%  \centering
%  \emph{Is there a red shape above a circle?} \\[1em]
%  \begin{tabular}{ccc}
%    \includegraphics[width=0.25\columnwidth]{fig/shapes1_big} &
%    \includegraphics[width=0.25\columnwidth]{fig/shapes2_big} &
%    \includegraphics[width=0.25\columnwidth]{fig/shapes3_big} \\
%    no & yes & yes
%  \end{tabular}
%  \caption{Example question, images, and answers from the \shapes dataset.
%    Answering this question requires being able to recognize an object
%    (\emph{circle}), an attribute (\emph{red}), and a spatial relation
%    (\emph{above}), and compare sets of objects previously retrieved.}
%\end{figure}


%\begin{figure}
%  \centering
%  \begin{tabular}{cc}
%    %\includegraphics[width=0.25\columnwidth]{fig/shapes_in1} &
%    %\includegraphics[width=0.25\columnwidth]{fig/shapes_out1} \\
%    \includegraphics[width=0.25\columnwidth]{fig/shapes_in2} &
%    \includegraphics[width=0.25\columnwidth]{fig/shapes_out2} \\
%  \end{tabular}
%\end{figure}

To produce an initial set of image features, we pass the input image through the
convolutional portion of a LeNet \cite{LeCun90LeNet} which is jointly trained
with the question-answering part of the model. We compare our approach to a
reimplementation of the VIS+LSTM baseline similar to the one described by
\cite{Ren15VQA}, again swapping out the pre-trained image embedding with a
LeNet.

\begin{table}
  \footnotesize
  \centering
  \begin{tabular}{ccccc}
    \toprule
    System & size 4 & size 5 & size 6 & All \\
    \midrule
    Majority & 64.4 & 62.5 & 61.7 & 63.0 \\
    VIS+LSTM & 71.9 & 62.5 & 61.7 & 65.3 \\
    NMN      & 89.7 & 92.4 & 85.2 & \bf 90.6 \\
    NMN (easy) & 97.7 & 91.1 & 89.7 & \bf 90.8 \\
    \bottomrule
  \end{tabular}
  \caption{Results on the \shapes dataset. Here ``size'' is the number of
    modules needed to instantiate an appropriate NMN. Our model achieves high
    accuracy and outperforms a baseline from previous work, especially on highly
    compositional questions. ``NMN (easy)''  is a modified training set with no
    size-6 questions; these results demonstrate that our model is able to
    generalize to questions more complicated than it has ever seen at training
    time.
  }
  \label{tab:shapes-results}
\end{table}

As can be seen in \autoref{tab:shapes-results}, our model achieves excellent
performance on this dataset, while the VIS+LSTM baseline fares little better
than a majority guesser. Moreover, the color detectors and attention
transformations behave as expected (\autoref{fig:shape-nmn}), indicating that
our joint training procedure correctly allocates responsibilities among modules.
This confirms that our approach is able to model complex compositional phenomena
outside the capacity of previous approaches to visual question answering. 

We perform an additional experiment on a modified version of the training set,
which contains no size-6 questions (i.e. questions whose corresponding NMN has 6
modules). Here our peformance doesn't suffer at all, and perhaps increases
slightly; this demonstrates that our model is able to generalize to questions
even more complicated than those it has seen during training. Using only
linguistic information, the model extrapolates simple visual patterns it has
learned to even harder questions.

\section{Experiments: natural images}

\begin{figure*}
  \centering
  \renewcommand{\arraystretch}{1.4}
  \begin{tabular}{>{\raggedright}*{5}{p{.17\textwidth}}}
    \toprule
    \includegraphics[width=\linewidth]{fig/thumb/cocoqa1.jpg} &
    \includegraphics[width=\linewidth]{fig/thumb/cocoqa2.jpg} &
    \includegraphics[width=\linewidth]{fig/shapes2_big} &
    \includegraphics[width=\linewidth]{fig/thumb/vqa1.jpg} &
    \includegraphics[width=\linewidth]{fig/thumb/vqa2.jpg} \\
    \emph{how many different lights in various different shapes and sizes?} &
    \emph{what is the color of the horse?} &
    \emph{is there a red shape above a circle?} &
    \emph{what color is the vase?} &
    \emph{is the bus full of passengers?} \\
    \midrule
    four (four) &
    brown (brown) &
    yes (yes) &
    green (green) &
    no (no) \\
    \bottomrule
    \\[1em]
    \toprule
    \includegraphics[width=\linewidth]{fig/thumb/cocoqa3.jpg} &
    \includegraphics[width=\linewidth]{fig/thumb/cocoqa4.jpg} &
    \includegraphics[width=\linewidth]{fig/bad_input_big} &
    \includegraphics[width=\linewidth]{fig/thumb/vqa3.jpg} &
    \includegraphics[width=\linewidth]{fig/thumb/vqa4.jpg} \\
    \emph{what is stuffed with toothbrushes wrapped in plastic?} & 
    \emph{where does the tabby cat watch a horse eating hay?} &
    \emph{is a red shape blue?} &
    \emph{what material are the boxes made of?} &
    \emph{is this a clock?} \\
    \midrule
    container (cup) &
    pen (barn) &
    yes (no) & 
    leather (cardboard) &
    yes (no) \\
    \bottomrule
  \end{tabular}
  \caption{Example output from our approach on different visual QA tasks. The
  top row shows correct answers, while the bottom row shows mistakes (correct
  answers are given in parentheses).}
  \label{fig:examples}
\end{figure*}

%\paragraph{\cocoqa}
%We also evaluate our system on the smaller \cocoqa dataset introduced by Ren
%XXX. This is similar to the VQA corpus, but uses questions
%automatically generated from captions rather than directly elicited from users.
%The \cocoqa dataset features four categories of question: objects, locations,
%colors and numbers. Our results are shown in Table XXX. As can be seen, the NMN
%model substantially outperforms the baselines in the color category, but is
%slightly worse in the others (this again appears to be due to overfitting).
%As above, the inputs come from a VGGNet without fine-tuning. Bad queries on this
%dataset are less attributable to parser errors, and more to badly-formed
%questions---because the dataset was generated by automatically rewriting
%captions, it is not uncommon to see totally incomprehensible questions like
%\emph{what does at sunset, upside down on the wet sand?}.
%
%\begin{table}
%  \footnotesize
%  \center
%  \begin{tabular}{lccccc}
%    \toprule
%    System & Object & Location & Color & Number & All \\
%    \midrule
%    IMG+BOW     & 58.7 & 49.4 & 52.0 & 44.1 & 55.9 \\
%    2-VIS+BLSTM & 58.2 & 47.3 & 49.5 & 44.8 & 55.1 \\
%    %NMN (indep) & 57.5 & 44.2 & 56.5 & 43.5 & 55.5 \\
%    NMN         & 57.9 & 48.8 & 54.3 & 40.5 & 55.5 \\
%    \bottomrule
%  \end{tabular}
%  \caption{\cocoqa results}
%\end{table}

Next we consider the model's ability to handle hard perceptual problems
involving natural images.  Here we evaluate on the recently-released VQA
dataset. This is the largest resource of its kind, consisting of more than
200,000 images, each paired with three questions and ten answers per question.
Data was generated by human annotators, in contrast to previous work, which has
generated questions automatically from captions \cite{Ren15VQA}.  We learn our
model using the standard train/test split, training only with those answers
marked as high confidence.  The visual input to the NMN is the conv5 layer of an
Oxford VGGNet \cite{Simonyan14VGG} after max-pooling. We do not fine-tune the
VGGNet.

Results are shown in \autoref{tab:vqa-results}. As can be seen, we outperform the best published
results on this task. A breakdown of our questions by answer type reveals that
our model performs especially well on questions answered by an object or
attribute, but worse than a sequence baseline on the other two categories.
Inspection of training-set accuracies suggests that performance in these
categories is due to overfitting. A hybrid system which falls back to an NMN for
these ``other'' category might achieve even better results; future work within
the NMN framework should focus on redesigning the \mod{classify-attention}
module so it is less prone to overfitting.

Inspection of parser outputs also suggests that there is substantial room to
improve the system using a better parser. A hand inspection of the first 40
50 parses in the training set suggests that most (80--90\%) of questions asking
for simple properties of objects are correctly analyzed, but more complicated questions are
more prone to picking up irrelevant predicates. For example \emph{are these
people most likely experiencing a work day?} is parsed as \mod{be(people,
likely)}, when the desired analysis is \mod{is(people, work)}. As noted above,
parser errors of this kind could be fixed with joint learning.

\autoref{fig:examples} is broadly suggestive of the kinds of prediction errors made by the
system, including plausible semantic confusions (cardboard interpreted as
leather, round windows interpreted as clocks), normal lexical variation
(\emph{container} for \emph{cup}), and use of answers that are \emph{a priori}
plausible but unrelated to the image (describing a horse as located in a pen
rather than a barn).

\begin{table}
  \footnotesize
  \center
  \begin{tabular}{lccccc}
    \toprule
    & \multicolumn{4}{c}{test-dev} & test \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-6}
    & Yes/No & Number & Other & All & All \\
    \midrule
    LSTM \cite{antol15iccv} & 78.20 & 35.7 & 26.6 & 48.8 & -- \\
    VIS+LSTM \cite{antol15iccv} & 78.9 & 35.2 & 36.4 & 53.7 & 54.1 \\
    NMN & 77.7 & 36.8 & 39.2 & 54.8 & \bf 55.1 \\
    \bottomrule
  \end{tabular}
  \caption{Results on the VQA corpus. Our model outperforms previous approaches,
    scoring particularly well on questions not involving a numeric or binary
    decision. Baseline numbers are as reported in previous work.}
    \label{tab:vqa-results}
\end{table}

