\section{Experiments: natural images}

\begin{figure*}
  \centering
  \begin{tabular}{*{5}{m{.17\textwidth}}}
    \includegraphics[width=\linewidth]{fig/cocoqa1.jpg} &
    \includegraphics[width=\linewidth]{fig/cocoqa2.jpg} &
    \includegraphics[width=\linewidth]{fig/shapes1_big} &
    \includegraphics[width=\linewidth]{fig/vqa1.jpg} &
    \includegraphics[width=\linewidth]{fig/vqa2.jpg} \\
    \emph{how many different lights in various different shapes and sizes?} &
    \emph{what is the color of the horse?} &
    \emph{is there a red shape above a circle?} &
    \emph{what color is the vase?} &
    \emph{is the bus full of passengers?} \\
    four (four) &
    brown (brown) &
    yes (yes) &
    green &
    no \\
    \midrule
    \includegraphics[width=\linewidth]{fig/cocoqa3.jpg} &
    \includegraphics[width=\linewidth]{fig/cocoqa4.jpg} &
    &
    \includegraphics[width=\linewidth]{fig/vqa3.jpg} &
    \includegraphics[width=\linewidth]{fig/vqa4.jpg} \\
    \emph{what is stuffed with toothbrushes wrapped in plastic?} & 
    \emph{where does the tabby cat watch a horse eating hay?} &
    ?? & 
    \emph{what material are the boxes made of?} &
    \emph{is this a clock?} \\
    container (cup) &
    pen (barn) &
    & 
    leather (cardboard) &
    yes (no) 
  \end{tabular}
  \caption{Example output from our approach on thre visual QA tasks.}
\end{figure*}

\paragraph{VQA}
We begin with a set of experiments on the recently-released VQA dataset. This
dataset consists of more than 200,000 images, each paired with three questions
and ten answers per question. Data was generated by XXX, more corpus flavor
text. We train our model using the standard train/test split, training only with
those answers marked as high confidence.
The input to each \mod{attend} module is the the Conv XXX layer of an Oxford
VGGNet (XXX) after max-pooling. We do not fine-tune the VGGNet.

Results are shown in Table XXX. As can be seen, we achieve state-of-the-art
results on this task. A breakdown of our questions by answer type reveals that
our model performs especially well on questions answered by an object or
attribute, but worse than a sequence baseline on the other two categories.
Inspection of training-set accuracies suggests that performance in these
categories is due to overfitting. A hybrid system which falls back to an NMN
for these ``other'' category achieves even better results; future work within
the NMN framework might focus on redesigning the \mod{classify-attention} module
so it is less prone to overfitting.

\begin{table}
  \footnotesize
  \center
  \begin{tabular}{lccccc}
    \toprule
    & \multicolumn{4}{c}{test-dev} & test \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-6}
    & Yes/No & Number & Other & All & All \\
    \midrule
    LSTM & 78.20 & 35.7 & 26.6 & 48.8 \\
    VIS+LSTM & 78.9 & 35.2 & 36.4 & 53.7 & 54.1 \\
    NMN & 77.7 & 36.8 & 39.2 & 54.8 & \bf 55.1 \\
    \bottomrule
  \end{tabular}
  \caption{VQA results}
\end{table}

\paragraph{\cocoqa}
We also evaluate our system on the smaller \cocoqa dataset introduced by Ren
XXX. This is similar in spirit to the VQA corpus, but uses questions
automatically generated from captions rather than directly elicited from users.
The \cocoqa dataset features four categories of question: objects, locations,
colors and numbers. Our results are shown in Table XXX. As can be seen, the NMN
model substantially outperforms the baselines in the color category, but is
slightly worse in the others (this again appears to be due to overfitting).
As above, the inputs come from a VGGNet without fine-tuning.

\begin{table}
  \footnotesize
  \center
  \begin{tabular}{lccccc}
    \toprule
    System & Object & Location & Color & Number & All \\
    \midrule
    IMG+BOW     & 58.7 & 49.4 & 52.0 & 44.1 & 55.9 \\
    2-VIS+BLSTM & 58.2 & 47.3 & 49.5 & 44.8 & 55.1 \\
    %NMN (indep) & 57.5 & 44.2 & 56.5 & 43.5 & 55.5 \\
    NMN         & 57.9 & 48.8 & 54.3 & 40.5 & 55.5 \\
    \bottomrule
  \end{tabular}
  \caption{\cocoqa results}
\end{table}

\section{Experiments: compositionality}

Past work [Ren paper] has achieved state-of-the-art results on the \cocoqa
dataset using a bag-of-words model. This is consistent with [VQA paper]'s
observation that image features are most important for simple object- and
activity-recognition questions, and that they make little difference for other
categories (like yes/no questions). Taken together, these results suggest that
existing natural image datasets primarily require simple object- and attribute
recognition, with limited importance attached to spatial relations and other
highly compositional phenomena.

As one of the primary goals of this work is to learn models for deep semantic
compositionality, we have created an additional dataset that places such
compositional phenomena at the forefront. This dataset consists of complex
questions about simple arrangements of colored shapes [XXX].  Questions contain
between two and four attributes, object types, or relationships.  There are 244
questions and 15616 images in total, divided into train and test sets (with no
overlap between the train and test sets).  To eliminate mode-guessing as a
viable strategy, all questions have a yes-or-no answer, but good performance
requires that the system learn to recognize shapes and colors, and understand
both spatial and logical relations among sets of objects.

XXX babyai

%\begin{figure}
%  \centering
%  \emph{Is there a red shape above a circle?} \\[1em]
%  \begin{tabular}{ccc}
%    \includegraphics[width=0.25\columnwidth]{fig/shapes1_big} &
%    \includegraphics[width=0.25\columnwidth]{fig/shapes2_big} &
%    \includegraphics[width=0.25\columnwidth]{fig/shapes3_big} \\
%    no & yes & yes
%  \end{tabular}
%  \caption{Example question, images, and answers from the \shapes dataset.
%    Answering this question requires being able to recognize an object
%    (\emph{circle}), an attribute (\emph{red}), and a spatial relation
%    (\emph{above}), and compare sets of objects previously retrieved.}
%\end{figure}


%\begin{figure}
%  \centering
%  \begin{tabular}{cc}
%    %\includegraphics[width=0.25\columnwidth]{fig/shapes_in1} &
%    %\includegraphics[width=0.25\columnwidth]{fig/shapes_out1} \\
%    \includegraphics[width=0.25\columnwidth]{fig/shapes_in2} &
%    \includegraphics[width=0.25\columnwidth]{fig/shapes_out2} \\
%  \end{tabular}
%\end{figure}

\begin{table}
  \footnotesize
  \centering
  \begin{tabular}{ccccc}
    \toprule
    System & Size 4 & Size 5 & Size 6 & All \\
    \midrule
    VIS+LSTM & 71.9 & 62.5 & 61.7 & 65.3 \\
    NMN      & 89.7 & 92.4 & 85.2 & \bf 90.6 \\
    NMN (easy) & 97.7 & 91.1 & 89.7 & \bf 90.8 \\
    \bottomrule
  \end{tabular}
  \caption{Synth data results}
\end{table}

As can be seen, our model achieves excellent performance on this dataset, while
competing approaches fare little better than the majority baseline. Moreover,
the color detectors and attention transformations behave as expected, indicating
that our joint training procedure correctly allocates responsibilities among
modules. Ultimately, in addition to achieving competitive results in answering
simple questions about natural images, our approach is able to model complex
compositional phenomena outside the capacity of previous approaches to visual
question answering.

