\section{Related work}

The idea of selecting a different network graph for each input datum is fundamental to both recurrent networks (where the network grows in the length of the input) \cite{Elman90RNN} and recursive neural networks (where the network is built, e.g., according to the syntactic structure of the input) \cite{Socher13CVG}. But both of these approaches ultimately involve repeated application of a single computational module (\eg an LSTM \cite{} or GRU \cite{} cell). Our basic contribution is in allowing the nodes of this graph to perform heterogeneous computations, and for "messages" of different kinds---raw image features, attentions, classification predictions---to be passed from one module to the next. We are unaware of any previous work allowing such mixed collections of modules to be trained jointly. 

\paragraph{non-image QA}
text only based: \cite{berant14acl,Liang13DCS}

\cite{geman15nas} a binary (yes/no) version of Visual Turing Test on synthetic data.

We are also unaware of past use of a semantic parser to predict network structures, or more generally to exploit the natural similarity between set-theoretic approaches to classical semantic parsing and attentional approaches to computer vision. As noted above, there is a large literature on learning to answer questions about structured knowledge representations from question--answer pairs, both with and without learning of meanings for simple predicates \cite{Liang13DCS,Krish2013Grounded}.

``neural'': \cite{iyyer14emnlp,weston14arxiv}

\paragraph{Grounding in images}
Deep Fragments \cite{karpathy14nips}, Flickr30k Entities \cite{plummer15iccv}.
Deep Visual-Semantic Alignments for Generating Image Descriptions \cite{karpathy15cvpr}.
(Image Retrieval using Scene Graphs \cite{johnson15cvpr})

\cite{Krish2013Grounded,matuszek12icml,kong14cvpr}

\paragraph{Datasets}
The present work is made possible by a number of recently-created corpora for question answering about images, including the DAQUAR dataset \cite{malinowski14nips}, the CocoQA dataset \cite{yu15arxiv}, and the VQA dataset \cite{antol15iccv}. 




\paragraph{image QA}
``classical'': \cite{malinowski14nips}

``neural'':
Several models for visual questioning have already been proposed in the literature \cite{ren2015image,ma15arxiv,gao2015you}, all of which use standard deep sequence modeling machinery to construct a joint embedding of image and text, which is immediately mapped to a distribution over answers. Here we attempt to more XXXly model the computational process needed to produce each answer, but also benefit from sequence and image embeddings XXX.