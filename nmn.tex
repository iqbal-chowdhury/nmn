\documentclass[10pt,twocolumn,letterpaper]{article}

% more motivation
% spatial heatmap / attention NOT detection
% how general is the parser? (where will it fail, are we losing perf., etc.)
% above vs not
% real figures for inplace

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{inconsolata}

\usepackage{array}
\usepackage{booktabs}
%\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}


%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{hyperref}
%\usepackage{url}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
%
%\usepackage{latexsym}
%\usepackage{amsmath,amsfonts}
%\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{relsize}
%\usepackage{mathtools}
%\usepackage{tikz}
%\usepackage{array}
%\usepackage{subcaption}
%\usepackage{comment}
%\usepackage{multirow}
%\usepackage{aliascnt}
%\usepackage{xspace}
%\usepackage[bb=fourier]{mathalfa}
%%\usepackage[font=small]{caption}
%\usepackage{siunitx}
%\usepackage{tablefootnote}
%\usepackage{multirow}
%\usepackage{microtype}
%\usepackage[export]{adjustbox}
%\usepackage{footmisc}

\include{defines}
% Include other packages here, before hyperref.

%\captionsetup{belowskip=8pt}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 %\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{1114} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\newcommand{\nmn}{Neural Module Networks\xspace}
\newcommand{\mod}[1]{{\small\texttt{#1}}}
\newcommand{\todo}{\textcolor{red}}
\newcommand{\shapes}{{\textsc{shapes}}\xspace}
\newcommand{\cocoqa}{{\textsc{CocoQA}}\xspace}
\newcommand{\marcus}[1]{\textcolor{blue}{Marcus: #1}}
%%%%%%%%% TITLE
\title{Deep Compositional Question Answering with Neural Module Networks}

\author{Jacob Andreas \and Marcus Rohrbach \and Trevor Darrell \and Dan Klein \\
Computer Science Division\\
University of California, Berkeley\\
{\tt\small \{jda,rohrbach,trevor,klein\}@cs.berkeley.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  Visual question answering is fundamentally compositional in nature---a
  question like \emph{where is the dog?} shares substructure with questions like
  \emph{what color is the dog?} and \emph{where is the cat?} This paper seeks to
  simultaneously exploit the representational capacity of deep networks and the
  compositional linguistic structure of questions.  We describe a procedure for
  constructing and learning \emph{neural module networks}, which compose
  collections of jointly-trained neural ``modules'' into deep networks for
  question answering. Our approach decomposes questions into their linguistic
  substructures, and uses these structures to dynamically instantiate modular
  networks (with reusable components for recognizing dogs, classifying colors,
  etc.). The resulting compound networks are jointly trained. We evaluate our
  approach on a variety of challenging datasets for visual question answering,
  achieving state-of-the-art results on both the VQA natural image dataset and a
  new dataset of complex questions about abstract shapes.
\end{abstract}

\input{intro}
\input{related}
\input{model}
\input{experiments}
\input{conclusion}

\end{document}
